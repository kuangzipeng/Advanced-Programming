{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'永定河孔雀城英国宫': '15500元/㎡', '绿宸·凤栖华府': '14500元/㎡', '中铁诺德春风和院': '67702元/㎡', '北京城建·宽院·国誉府': '31136元/㎡', '燕西华府': '380万元/套起', '建邦·顺颐府': '55583元/㎡', '萬橡悦府': '54197元/㎡', '观承望溪': '55711元/㎡', '北京城建·世华龙樾': '82600元/㎡', '国誉府': '43467元/㎡', '融创·崇礼公馆': '18000元/㎡', '中新健康城·中新悦朗': '8200元/㎡', '台湖金茂悦': '51585元/㎡'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from pyquery import PyQuery as pq\n",
    "\n",
    "def get_NewHouse(url):\n",
    "    r=requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = 'utf-8'\n",
    "    html=r.text\n",
    "    doc=pq(html)   \n",
    "    lis=doc('.nlcd_name')   \n",
    "    price=doc('.nhouse_price')   \n",
    "    name_list=[]\n",
    "    price_list=[]\n",
    "    for a in lis:\n",
    "        name_list.append(pq(a).text())\n",
    "    for span in price:\n",
    "        price_list.append(pq(span).text())\n",
    "    \n",
    "    list=[name_list,price_list]\n",
    "    output= {}\n",
    "    output=dict(zip(name_list, price_list))\n",
    "    print(output)\n",
    "\n",
    "    \n",
    "def getURL(url):\n",
    "    res = requests.get(url)\n",
    "    res.encoding = 'gb2312'\n",
    "    html = res.text            #获取网页内容\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    lis = soup.find('li',class_=\"quyu_name dingwei\")\n",
    "    url= BeautifulSoup(str(lis),\"lxml\")\n",
    "    url_lis = url.find_all('a')\n",
    "    pre='https://newhouse.fang.com'\n",
    "    distrcit_link = []\n",
    "    for i in range(1,19):\n",
    "        links=pre+url_lis[i]['href']\n",
    "        distrcit_link.append(links)\n",
    "    num=0\n",
    "    for i in distrcit_link:\n",
    "        crawlFang(i,num)   #num用于区分各区网址\n",
    "        num=num+1\n",
    "\n",
    "\n",
    "def crawlFang(url,num): #定义一个爬取字段的函数    \n",
    "    district=['chaoyang','haidian','fengtai','xicheng','dongcheng','changping','daxing','tongzhou','fangshan','shunyi','shijingshan','miyun','mentougou','huairou','yanqing','pinggu','yanjiao','beijingzhoubian']\n",
    "    namelis=[]\n",
    "    addresslis=[]\n",
    "    infolis=[]\n",
    "    statuslis=[]    \n",
    "    pre='https://newhouse.fang.com/house/s/'\n",
    "    NextPage_link = [] \n",
    "    if(num==1 or num==2 or num==8): #两页\n",
    "        for i in range(1,3):\n",
    "            links=pre+district[num]+'/b9'+str(i)\n",
    "            NextPage_link.append(links)\n",
    "    elif(num==6): #四页\n",
    "        for i in range(1,5):\n",
    "            links=pre+district[num]+'/b9'+str(i)\n",
    "            NextPage_link.append(links)\n",
    "    elif(num==0 or num==5 or num==7 or num==9):\n",
    "        for i in range(1,4): #三页\n",
    "            links=pre+district[num]+'/b9'+str(i)\n",
    "            NextPage_link.append(links)\n",
    "    else:\n",
    "        links=pre+district[num]+'/b91'\n",
    "        NextPage_link.append(links)\n",
    "        \n",
    "    for i in NextPage_link:       \n",
    "        r = requests.get(i)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = 'gb2312'\n",
    "        web_con=r.text\n",
    "        doc=pq(web_con)\n",
    "        name=doc('.nlcd_name')\n",
    "        for a in name:\n",
    "            namelis.append((pq(a).text()))\n",
    "            \n",
    "        info = doc('.house_type')\n",
    "        for b in info:\n",
    "            infolis.append((pq(b).text()))\n",
    "    \n",
    "        address=doc('.address')\n",
    "        for c in address:\n",
    "            addresslis.append((pq(c).text()))\n",
    "    \n",
    "        status = doc('.fangyuan')\n",
    "        for d in status:\n",
    "            statuslis.append((pq(d).text()))    \n",
    "        \n",
    "    writefile(namelis,addresslis,infolis,statuslis,num)\n",
    "\n",
    "    \n",
    "def writefile(namelis,addresslis,infolis,statuslis,num):\n",
    "    location=['朝阳','海淀','丰台','西城','东城','昌平','大兴','通州','房山','顺义','石景山','密云','门头沟','怀柔','延庆','平谷','燕郊','北京周边']     \n",
    "    with open(location[num]+'.csv','w',newline='',encoding='utf-8-sig') as csvfile:\n",
    "        writer=csv.writer(csvfile)\n",
    "        writer.writerow(['楼盘','具体位置','户型','楼盘状态'])\n",
    "        count=0\n",
    "        for j in namelis:\n",
    "            writer.writerow([namelis[count],addresslis[count],infolis[count],statuslis[count]]) #括号里为要写入csv文档的对象\n",
    "            count=count+1\n",
    "        csvfile.close()#关闭csv\n",
    "\n",
    "    \n",
    "NewHouse_url=\"https://newhouse.fang.com/house/saledate/201911.htm\"\n",
    "get_NewHouse(NewHouse_url)\n",
    "url = \"https://newhouse.fang.com/house/s/\"\n",
    "getURL(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
